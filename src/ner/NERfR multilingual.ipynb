{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj7uA0qGZIHH"
      },
      "source": [
        "# NERfR (Named Entity Recognition for Recipes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ysByC8eoUBWK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pgajo/working/food/food-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text_en': '5 ounces rum;4 ounces triple sec;3 ounces Tia Maria;20 ounces orange juice', 'entities_en': [[0, 1, 'QUANTITY'], [2, 8, 'UNIT'], [9, 12, 'FOOD'], [13, 14, 'QUANTITY'], [15, 21, 'UNIT'], [22, 32, 'FOOD'], [33, 34, 'QUANTITY'], [35, 41, 'UNIT'], [42, 51, 'FOOD'], [52, 54, 'QUANTITY'], [55, 61, 'UNIT'], [62, 74, 'FOOD']], 'text_it': \"5 once rum;4 once triple sec;3 once Tia Maria;20 once succo d'arancia\", 'entities_it': [[0, 1, 'QUANTITY'], [2, 6, 'UNIT'], [7, 10, 'FOOD'], [11, 12, 'QUANTITY'], [13, 17, 'UNIT'], [18, 28, 'FOOD'], [29, 30, 'QUANTITY'], [31, 35, 'UNIT'], [36, 45, 'FOOD'], [46, 48, 'QUANTITY'], [49, 53, 'UNIT'], [54, 69, 'FOOD']]}\n"
          ]
        }
      ],
      "source": [
        "# Installations and Imports\n",
        "import spacy\n",
        "import sys\n",
        "from tasteset_utils import prepare_data, ENTITIES\n",
        "import json\n",
        "# json_path = '/home/pgajo/working/food/data/TASTEset/data/TASTEset_semicolon_formatted_en-it_unaligned_aligned_model=mdeberta-v3-base-xl-wa_recipe_aligner_5epochs_error_rate=0.0119_pruned.json'\n",
        "json_path = '/home/pgajo/working/food/data/TASTEset/data/TASTEset_semicolon_formatted_en-it_itemwise.json'\n",
        "\n",
        "with open(json_path, 'r') as f:\n",
        "    training_data = json.load(f)\n",
        "print(training_data['annotations'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2UHaCcw_cKi"
      },
      "outputs": [],
      "source": [
        "# make spacy dataset\n",
        "from spacy.tokens import DocBin\n",
        "import os\n",
        "import re\n",
        "\n",
        "def doc_from_annotations(training_annotations, languages = ['en']):\n",
        "  num_of_entities = 0\n",
        "  doc_bin = DocBin()\n",
        "  for lang in languages:\n",
        "    nlp = spacy.blank(lang)\n",
        "    for idx, example in enumerate(training_annotations):\n",
        "      doc = nlp.make_doc(re.sub(r'-(\\d+)', r' \\1', example[f'text_{lang}'].replace(';', ' ')))\n",
        "      print(idx, 'doc:', doc)\n",
        "      ents = []\n",
        "      # print(\"len(example[f'entities_{lang}']):\", len(example[f'entities_{lang}']))\n",
        "      for i, entity in enumerate(example[f'entities_{lang}']):\n",
        "        span = doc.char_span(*entity, alignment_mode='strict')\n",
        "        # print('span.start', span.start)\n",
        "        # print('span.end', span.end)\n",
        "        # print(i, 'entity\\t', entity, '\\tspan:\\t', span)\n",
        "        # print(i, 'entity\\t', entity, '\\traw:\\t', example[f'text_{lang}'][entity[0]:entity[1]])\n",
        "        # if the span is None, skip it and don't add it to the doc's entities\n",
        "        if span is None:\n",
        "          continue\n",
        "        ents.append(span)\n",
        "        num_of_entities += 1\n",
        "      \n",
        "      doc.ents = ents\n",
        "      doc_bin.add(doc)\n",
        "    \n",
        "  print('num_of_entities:', num_of_entities)\n",
        "  return doc_bin\n",
        "\n",
        "train_len = int(0.8*len(training_data['annotations'])) # 80/20 split\n",
        "languages = ['it']\n",
        "lang_id = '-'.join(languages)\n",
        "train_bin = doc_from_annotations(training_data['annotations'][:train_len], languages = languages)\n",
        "print('train_bin length:', len(train_bin))\n",
        "dev_bin = doc_from_annotations(training_data['annotations'][train_len:], languages = languages)\n",
        "print('dev_bin length:', len(dev_bin))\n",
        "spacy_dir = '/home/pgajo/working/food/data/TASTEset/data/spacy'\n",
        "train_path = os.path.join(spacy_dir, f\"{lang_id}_train.spacy\")\n",
        "dev_path = os.path.join(spacy_dir, f\"{lang_id}_dev.spacy\")\n",
        "train_bin.to_disk(train_path)\n",
        "dev_bin.to_disk(dev_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer config\n",
        "model_name = 'bert-base-multilingual-cased'\n",
        "BASE_CONFIG_TRANFORMER = \"\"\"\n",
        "# This is an auto-generated partial config. To use it with 'spacy train'\n",
        "# you can run spacy init fill-config to auto-fill all default settings:\n",
        "# python -m spacy init fill-config ./base_config.cfg ./config.cfg\n",
        "[paths]\n",
        "train = train_path\n",
        "dev = dev_path\n",
        "vectors = null\n",
        "[system]\n",
        "gpu_allocator = \"pytorch\"\n",
        "\n",
        "[nlp]\n",
        "lang = \"it\"\n",
        "pipeline = [\"transformer\",\"ner\"]\n",
        "batch_size = 128\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.transformer]\n",
        "factory = \"transformer\"\n",
        "\n",
        "[components.transformer.model]\n",
        "@architectures = \"spacy-transformers.TransformerModel.v3\"\n",
        "name = \"model_name\"\n",
        "# name = \"microsoft/mdeberta-v3-base\"\n",
        "tokenizer_config = {\"use_fast\": true}\n",
        "\n",
        "[components.transformer.model.get_spans]\n",
        "@span_getters = \"spacy-transformers.strided_spans.v1\"\n",
        "window = 128\n",
        "stride = 96\n",
        "\n",
        "[components.ner]\n",
        "factory = \"ner\"\n",
        "\n",
        "[components.ner.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"ner\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 64\n",
        "maxout_pieces = 2\n",
        "use_upper = false\n",
        "nO = null\n",
        "\n",
        "[components.ner.model.tok2vec]\n",
        "@architectures = \"spacy-transformers.TransformerListener.v1\"\n",
        "grad_factor = 1.0\n",
        "\n",
        "[components.ner.model.tok2vec.pooling]\n",
        "@layers = \"reduce_mean.v1\"\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 0\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "\n",
        "[training]\n",
        "accumulate_gradient = 3\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "\n",
        "[training.optimizer.learn_rate]\n",
        "@schedules = \"warmup_linear.v1\"\n",
        "warmup_steps = 250\n",
        "total_steps = 5000\n",
        "initial_rate = 2e-5\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_padded.v1\"\n",
        "discard_oversize = true\n",
        "size = 2000\n",
        "buffer = 256\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}\"\"\"\n",
        "BASE_CONFIG_TRANFORMER = BASE_CONFIG_TRANFORMER.replace('model_name', model_name)\n",
        "BASE_CONFIG_TRANFORMER = BASE_CONFIG_TRANFORMER.replace('train_path', train_path)\n",
        "BASE_CONFIG_TRANFORMER = BASE_CONFIG_TRANFORMER.replace('dev_path', dev_path)\n",
        "print(BASE_CONFIG_TRANFORMER)\n",
        "base_config_path = f\"{model_name}.cfg\"\n",
        "with open(base_config_path, 'w') as f:\n",
        "  f.write(BASE_CONFIG_TRANFORMER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7HSLGd-Bx0e",
        "outputId": "c1dbe6a9-c220-427b-a19a-00529ab83310"
      },
      "outputs": [],
      "source": [
        "# This command fills in your config with from the base_config you generated. The\n",
        "# last argument is the name of your config. I used \"_eff\" for \"efficiency\". Feel\n",
        "# free to change that\n",
        "# !python -m spacy init fill-config mbert.cfg config_mbert.cfg\n",
        "model_config_path = f\"config_{model_name}.cfg\"\n",
        "!python -m spacy init fill-config \"$base_config_path\" \"$model_config_path\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcdUi-uag79A"
      },
      "source": [
        "## Training\n",
        "\n",
        "Run the following code to train! Note that you'll have to change the path and name of the `.cfg` file as necessary. The last argument is a folder that'll contain your pipeline. Feel free to prefix it with a path to a more useful location. Also have some fun with the name!\n",
        "\n",
        "You'll get periodic updates with the `loss`, `F1`, `precision`, `recall` for the NER model over time. They also give you a `SCORE`, which is helpful when training multiple components, but in our case, the `SCORE` is just the `F1` score for the NER model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGK-9GucDUqQ",
        "outputId": "3c341506-f3ef-4b69-f8db-084b79bcdd4a"
      },
      "outputs": [],
      "source": [
        "suffix = 'item-wise'\n",
        "output_path = f\"output_{model_name}_{lang_id}_{suffix}\"\n",
        "!python -m spacy train \"$model_config_path\" --output \"$output_path\" -g 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp9fYB6CfZQR"
      },
      "source": [
        "## Results\n",
        "\n",
        "The training outputs a `meta.json` file in the output folder (`output_eff` in our case). We can use this to check a number of metrics, including the performance of each entity class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PckfQRDo4o8S"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# grab the performance dict from within the meta file\n",
        "print(f\"{output_path}/model-best/meta.json\")\n",
        "performance = json.load(open(f\"{output_path}/model-best/meta.json\", 'r'))['performance']\n",
        "performance_by_ent = performance['ents_per_type']\n",
        "\n",
        "perf_df = pd.DataFrame(performance_by_ent)\n",
        "perf_df[\"TOTAL\"] = [performance['ents_p'], performance['ents_r'], performance['ents_f']]\n",
        "# sort by header\n",
        "perf_df = perf_df.reindex(sorted(perf_df.columns), axis=1)\n",
        "\n",
        "# display df with the cell color corresponding to the value (dark=high; light=low)\n",
        "perf_df.style.background_gradient(\n",
        "    axis=1, low=perf_df.min().min(), high=1, cmap='YlOrBr'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lqB7YOzknk3"
      },
      "source": [
        "Here we've got the precision (p), recall (r), and F1 (f) score by entity. It seems like the best performing entities are the ones we care the most about. Only 40% of *PART* entities are being turned up. I can live with that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHzHar66xZG5"
      },
      "source": [
        "## Getting the Confusion Matrix\n",
        "\n",
        "We're going to be plotting a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) on the same test set we used for training. At a high level, this entails running each sample through the trained model, and, for each token, storing the entity the model predicted for that token, as well as the ground truth entity (as labeled by the dataset authors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrIleK5oxrHu"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "# load the model and test set. Again, change the paths as required\n",
        "nlp = spacy.load(f\"{output_path}/model-best\")\n",
        "test_set = list(DocBin().from_disk(dev_path).get_docs(nlp.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glbxDfxJB4h0"
      },
      "outputs": [],
      "source": [
        "pred_ents = []\n",
        "true_ents = []\n",
        "\n",
        "for recipe in test_set:\n",
        "  # tok.ent_type_ gets the ent per token, as opposed to breaking the Doc into\n",
        "  # entities. This ensures that `true_ents` and `pred_ents` are the same length.\n",
        "  true_ents += [tok.ent_type_ for tok in recipe]\n",
        "  # `recipe.text` grabs the raw recipe, because `recipe` already contains entity\n",
        "  # labels.\n",
        "  pred_ents += [tok.ent_type_ for tok in nlp(recipe.text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "DChFnKdSGqwV",
        "outputId": "4cab256d-9bb4-4811-f9f7-acd0e6ef856e"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# create and display the confusion matrix\n",
        "cm = confusion_matrix(true_ents, pred_ents, labels=ENTITIES)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ENTITIES)\n",
        "\n",
        "disp.plot()\n",
        "plt.xticks(rotation=70)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRNNbNy9-F7o"
      },
      "source": [
        "Unfortunately, there isn't quite enough data for the color mapping to show fine-grained differences. Nonetheless, we can get a sense for the more common mislabelings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCiKVxbf_x8y"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's test this baby out!\n",
        "\n",
        "You'll notice I load the recipes as lists of ingredient lines and run each ingredient line through the pipeline separately, even though we trained the models on recipes\n",
        "I found it performed better this way, despite the formatting mismatch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-VsLEqiljQU",
        "outputId": "250a7d4b-7863-4e9e-de6c-8dd127c311b9"
      },
      "outputs": [],
      "source": [
        "!pip install recipe-scrapers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9THhIHxmGhI",
        "outputId": "235cd389-0c7b-4291-a4ce-33b9b26df59d"
      },
      "outputs": [],
      "source": [
        "from recipe_scrapers import scrape_me\n",
        "\n",
        "# RECIPE_URL = \"https://ricette.giallozafferano.it/Trota-salmonata-in-crosta-di-pistacchi.html\"\n",
        "RECIPE_URL = \"https://cucchiaio.it/ricetta/torta-con-farina-di-mandorle/\"\n",
        "scraper = scrape_me(RECIPE_URL)\n",
        "scraper.ingredients()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsxu4HGxWBpQ"
      },
      "source": [
        "### Text Pre-Processing Function\n",
        "\n",
        "Note the float representations of ingredient quantities, in spite of the fact that the website shows them in mixed numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ES2pKiK0NOE"
      },
      "outputs": [],
      "source": [
        "from fractions import Fraction\n",
        "import re\n",
        "\n",
        "\n",
        "def fraction_to_mixed_number(fraction: Fraction) -> str:\n",
        "  if fraction.numerator >= fraction.denominator:\n",
        "    whole, remainder = divmod(fraction.numerator, fraction.denominator)\n",
        "    if remainder == 0:\n",
        "      return str(whole)\n",
        "    else:\n",
        "      return f\"{whole} {Fraction(remainder, fraction.denominator)}\"\n",
        "  else:\n",
        "    return str(fraction)\n",
        "\n",
        "\n",
        "def convert_floats_to_fractions(text: str) -> str:\n",
        "    return re.sub(\n",
        "        r'\\b-?\\d+\\.\\d+\\b',\n",
        "        lambda match: fraction_to_mixed_number(\n",
        "            Fraction(float(match.group())).limit_denominator()), text\n",
        "        )\n",
        "\n",
        "\n",
        "def process_text(text, model=nlp):\n",
        "  \"\"\"\n",
        "  A wrapper function to pre-process text and run it through our pipeline.\n",
        "  \"\"\"\n",
        "  return nlp(convert_floats_to_fractions(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1IbSIiFycKI",
        "outputId": "60fb0cff-4b40-4d7a-e6f1-719259aba42e"
      },
      "outputs": [],
      "source": [
        "# Let's have a look at our processing function at work\\\n",
        "fraction_mapping = { \n",
        "        '½': '1/2', '¼': '1/4', '¾': '3/4',\n",
        "        '⅓': '1/3', '⅔': '2/3', '⅕': '1/5',\n",
        "        '⅖': '2/5', '⅗': '3/5', '⅘': '4/5',\n",
        "        '⅙': '1/6', '⅚': '5/6', '⅛': '1/8',\n",
        "        '⅜': '3/8', '⅝': '5/8', '⅞': '7/8',\n",
        "    }\n",
        "import re\n",
        "def convert_single_char_fractions(text):\n",
        "    for key in fraction_mapping.keys():\n",
        "        text = text.replace(key, fraction_mapping[key])\n",
        "    return text\n",
        "\n",
        "[convert_single_char_fractions(convert_floats_to_fractions(line)) for line in scraper.ingredients()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6rbxA_fXPnE"
      },
      "source": [
        "### Running Inference with Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJHRbPaAXPBM"
      },
      "outputs": [],
      "source": [
        "# Load the model again for good measure\n",
        "nlp = spacy.load(f\"{output_path}/model-best/\")\n",
        "\n",
        "from spacy import displacy\n",
        "# process the recipe, line-by-line\n",
        "docs = [process_text(line, model = nlp) for line in scraper.ingredients()]\n",
        "\n",
        "displacy.render(docs, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PREPPY_URL = 'https://preppykitchen.com/coffee-cake/'\n",
        "scraper = scrape_me(PREPPY_URL, wild_mode=True)\n",
        "scraper.ingredients()\n",
        "# process the recipe, line-by-line\n",
        "docs_preppy = [process_text(line) for line in scraper.ingredients()]\n",
        "\n",
        "displacy.render(docs_preppy, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
