{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgajo/working/food/food-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.set_printoptions(linewidth=10000)\n",
    "tokenizer_name = 'bert-base-multilingual-cased'\n",
    "# tokenizer_name = 'microsoft/mdeberta-v3-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "# model_name = 'pgajo/mbert-xl-wa-tasteset-recipe-aligner'\n",
    "# model_name = 'pgajo/mdeberta-v3-base-recipe-aligner'\n",
    "# model_name = 'pgajo/mdeberta-v3-base-xl-wa-tasteset-recipe-aligner'\n",
    "# model_name = 'pgajo/mdeberta-v3-base-xl-wa_TASTEset_20240101-140212'\n",
    "model_name = 'pgajo/bert-base-multilingual-cased-recipe-aligner-en-it-3-epochs'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bilingual version of the json dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# path = '/home/pgajo/working/food/data/TASTEset/data/TASTEset_sep_format.json'\n",
    "# with open(path, encoding='utf8') as f:\n",
    "#     data = json.load(f)\n",
    "# print(data)\n",
    "\n",
    "# with open('/home/pgajo/working/food/data/TASTEset/data/TASTEset_sep_format_raw.nofractions.it', 'r', encoding='utf8') as f:\n",
    "#     italian_recipes = f.readlines()\n",
    "# bilingual_data = []\n",
    "# for i, entry in enumerate(data['annotations']):\n",
    "#     new_entry = {}\n",
    "#     new_entry['text_en'] = entry['text_en']\n",
    "#     new_entry['entities_en'] = entry['entities_en']\n",
    "#     new_entry['text_it'] = italian_recipes[i].strip()\n",
    "#     new_entry['entities_it'] = []\n",
    "#     bilingual_data.append(new_entry)\n",
    "# bilingual_dataset = {'classes': data['classes'], 'annotations': bilingual_data}\n",
    "# print(bilingual_dataset)\n",
    "\n",
    "# save bilingual dataset to json\n",
    "# bilingual_path = path.replace('.json', '_en-it_unaligned.json')\n",
    "# with open(bilingual_path, 'w', encoding='utf8') as f:\n",
    "    # json.dump(bilingual_dataset, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text_en': '5 ounces rum;4 ounces triple sec;3 ounces Tia Maria;20 ounces orange juice', 'entities_en': [[0, 1, 'QUANTITY'], [2, 8, 'UNIT'], [9, 12, 'FOOD'], [13, 14, 'QUANTITY'], [15, 21, 'UNIT'], [22, 32, 'FOOD'], [33, 34, 'QUANTITY'], [35, 41, 'UNIT'], [42, 51, 'FOOD'], [52, 54, 'QUANTITY'], [55, 61, 'UNIT'], [62, 74, 'FOOD']], 'text_it': \"5 once di rum;4 once di triple sec;3 once di Tia Maria;20 once di succo d'arancia\", 'entities_it': []}, {'text_en': '2 tubes cinnamon roll, refrigerated, with icing;4 tablespoons butter, melted;6 eggs;1/2 cup milk;2 teaspoons cinnamon;2 teaspoons vanilla;1 cup maple syrup', 'entities_en': [[0, 1, 'QUANTITY'], [2, 7, 'UNIT'], [8, 21, 'FOOD'], [23, 35, 'PROCESS'], [37, 41, 'FOOD'], [42, 47, 'FOOD'], [48, 49, 'QUANTITY'], [50, 61, 'UNIT'], [62, 68, 'FOOD'], [70, 76, 'PROCESS'], [77, 78, 'QUANTITY'], [79, 83, 'FOOD'], [84, 87, 'QUANTITY'], [88, 91, 'UNIT'], [92, 96, 'FOOD'], [97, 98, 'QUANTITY'], [99, 108, 'UNIT'], [109, 117, 'FOOD'], [118, 119, 'QUANTITY'], [120, 129, 'UNIT'], [130, 137, 'FOOD'], [138, 139, 'QUANTITY'], [140, 143, 'UNIT'], [144, 155, 'FOOD']], 'text_it': \"2 tubi di cinnamon roll, refrigerati, con glassa;4 cucchiai di burro, fuso;6 uova;1/2 tazza di latte;2 cucchiaini di cannella;2 cucchiaini di vaniglia;1 tazza di sciroppo d'acero\", 'entities_it': []}, {'text_en': '4 ripe coconuts;1 cup evaporated milk;1 cup gin;3 tablespoons sugar (optional);1 teaspoon ground cinnamon;1/2 teaspoon freshly grated nutmeg', 'entities_en': [[0, 1, 'QUANTITY'], [2, 6, 'PHYSICAL_QUALITY'], [7, 15, 'FOOD'], [16, 17, 'QUANTITY'], [18, 21, 'UNIT'], [22, 37, 'FOOD'], [38, 39, 'QUANTITY'], [40, 43, 'UNIT'], [44, 47, 'FOOD'], [48, 49, 'QUANTITY'], [50, 61, 'UNIT'], [62, 67, 'FOOD'], [79, 80, 'QUANTITY'], [81, 89, 'UNIT'], [90, 96, 'PROCESS'], [97, 105, 'FOOD'], [106, 109, 'QUANTITY'], [110, 118, 'UNIT'], [119, 133, 'PROCESS'], [134, 140, 'FOOD']], 'text_it': '4 noci di cocco mature;1 tazza di latte evaporato;1 tazza di gin;3 cucchiai di zucchero (facoltativo);1 cucchiaino di cannella macinata;1/2 cucchiaino di noce moscata fresca grattugiata', 'entities_it': []}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "bilingual_path = '/home/pgajo/working/food/data/TASTEset/data/formatted data/TASTEset_sep_format_en-it_unaligned.json'\n",
    "with open(bilingual_path, encoding='utf8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data['annotations'][:3])\n",
    "\n",
    "recipe_list = data['annotations']#[:3]\n",
    "len(recipe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 14/700 [00:04<03:35,  3.19it/s, errors=34, error rate=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: CHAR SPAN PREDICTION DOES NOT MATCH TOKEN-BASED PREDICTION\n",
      "recipe no.: 14, entity no.: 12\n",
      "garlic\n",
      "1 barattolo di fagioli di garbanzo, scolati e sciacquati;1/4 di cucchiaino di sale;1 spicchio d'aglio;1/2 cucchiaino di cumino macinato;1/2 tazza di pesto di basilico;1 cucchiaio di succo di limone;2 cucchiai d'acqua\n",
      "span_number: 2\n",
      "search_span: [83, 101]\n",
      "prediction_shift: 83\n",
      "search_span_context: 1 spicchio d'aglio\n",
      "query_tokens: tensor([  101, 47243, 25303,   102])\n",
      "len(query_tokens): 4\n",
      "start_index_token: 30\n",
      "model_token_shift: 0\n",
      "end_index_token: 33\n",
      "len(input_ids): 76\n",
      "encoding: tensor([  101, 10208, 54434, 47243, 10927, 12096, 10347, 15008,   117,   122, 10944,   117, 68507, 28751, 10111, 42840, 16219,   132,   122,   120,   125, 57675, 54609, 15938, 44253,   132,   122,   171, 73477,  1729, 47243, 25303,  1729,   132,   122,   120,   123, 57675, 54609, 15938, 16912, 16008, 10245,   132,   122,   120,   123, 41506, 82300, 10161, 59411, 10340,   132,   122, 61939, 74378, 10141, 16278, 23005, 11918,   132,   123, 61939, 74378, 10107, 12286,   102,   122, 32650, 11130, 52701,   172,   112, 17147, 10133,   102], device='cuda:0')\n",
      "decoded: (0, [CLS]) (1, 15) (2, oz) (3, gar) (4, ##ban) (5, ##zo) (6, be) (7, ##ans) (8, ,) (9, 1) (10, can) (11, ,) (12, dra) (13, ##ined) (14, and) (15, rin) (16, ##sed) (17, ;) (18, 1) (19, /) (20, 4) (21, tea) (22, ##sp) (23, ##oon) (24, salt) (25, ;) (26, 1) (27, c) (28, ##love) (29, •) (30, gar) (31, ##lic) (32, •) (33, ;) (34, 1) (35, /) (36, 2) (37, tea) (38, ##sp) (39, ##oon) (40, ground) (41, cum) (42, ##in) (43, ;) (44, 1) (45, /) (46, 2) (47, cup) (48, basi) (49, ##l) (50, pes) (51, ##to) (52, ;) (53, 1) (54, tables) (55, ##poon) (56, le) (57, ##mon) (58, ju) (59, ##ice) (60, ;) (61, 2) (62, tables) (63, ##poon) (64, ##s) (65, water) (66, [SEP]) (67, 1) (68, sp) (69, ##ic) (70, ##chio) (71, d) (72, ') (73, agli) (74, ##o) (75, [SEP])\n",
      "prediction_tokens: tensor([47243, 25303,  1729], device='cuda:0')\n",
      "token_based_prediction: ['garlic •']\n",
      "token_based_prediction_splitjoined: ['garlic•']\n",
      "gold: ['garlic']\n",
      "char_span_start CharSpan(start=76, end=79)\n",
      "char_span_start_adjusted 159\n",
      "char_span_end CharSpan(start=83, end=84)\n",
      "char_span_end_adjusted: 167\n",
      "char_span_prediction: ['asilico;']\n",
      "char_span_prediction_splitjoined: ['asilico;']\n",
      "full context_en: 15 oz garbanzo beans, 1 can, drained and rinsed;1/4 teaspoon salt;1 clove garlic;1/2 teaspoon ground cumin;1/2 cup basil pesto;1 tablespoon lemon juice;2 tablespoons water\n",
      "full context_it: 1 barattolo di fagioli di garbanzo, scolati e sciacquati;1/4 di cucchiaino di sale;1 spicchio d'aglio;1/2 cucchiaino di cumino macinato;1/2 tazza di pesto di basilico;1 cucchiaio di succo di limone;2 cucchiai d'acqua\n",
      "error_count: 38\n",
      "sample_count: 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "error_count = 0\n",
    "sample_count = 0\n",
    "inference_progress_bar = tqdm(enumerate(recipe_list), total=len(recipe_list))\n",
    "for idx, recipe in inference_progress_bar:\n",
    "    # print(idx, '++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    recipe_ingredient_strings_en = recipe['text_en'].split(';')\n",
    "    recipe_ingredient_spans_en = []\n",
    "    for ingredient_string_en in recipe_ingredient_strings_en:\n",
    "        start_index = recipe['text_en'].find(ingredient_string_en)\n",
    "        end_index = start_index + len(ingredient_string_en)\n",
    "        recipe_ingredient_spans_en.append([start_index, end_index])\n",
    "    # print(recipe_ingredient_spans_en)\n",
    "    recipe_ingredient_strings_it = recipe['text_it'].split(';')\n",
    "    recipe_ingredient_spans_it = []\n",
    "    for ingredient_string_it in recipe_ingredient_strings_it:\n",
    "        start_index = recipe['text_it'].find(ingredient_string_it)\n",
    "        end_index = start_index + len(ingredient_string_it)\n",
    "        recipe_ingredient_spans_it.append([start_index, end_index])\n",
    "    # print(recipe_ingredient_spans_it)\n",
    "\n",
    "    for i, entity in enumerate(recipe['entities_en']):\n",
    "        sample_count += 1\n",
    "        span_number = -1\n",
    "        prediction_shift = 0\n",
    "\n",
    "        # find in which ingredient span the entity is\n",
    "        for j, span in enumerate(recipe_ingredient_spans_en):\n",
    "            if entity[0] >= span[0] and entity[1] <= span[1]:\n",
    "                span_number = j\n",
    "                break\n",
    "        \n",
    "        # define a shift to account for the length of the previous ingredient spans\n",
    "        for j, span in enumerate(recipe_ingredient_spans_en[:span_number]):\n",
    "            prediction_shift += len(recipe_ingredient_strings_it[j]) + 1\n",
    "\n",
    "        search_span = recipe_ingredient_spans_it[span_number]\n",
    "        search_span_context = recipe['text_it'][search_span[0]:search_span[1]]\n",
    "        input = tokenizer(\n",
    "            # recipe['text_en'][entity[0]:entity[1]], # query\n",
    "            # '• ' + recipe['text_en'][entity[0]:entity[1]] + ' •', # single-entity query with delimiters\n",
    "            recipe['text_en'][:entity[0]] + '• ' + recipe['text_en'][entity[0]:entity[1]] + ' •' + recipe['text_en'][entity[1]:], # full-sentence query with delimiters\n",
    "                          search_span_context, # context\n",
    "                          return_tensors='pt',\n",
    "                          ).to(device)\n",
    "        input_ids = input['input_ids'].squeeze()\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**input)\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "        start_index_token = int(torch.argmax(start_scores))\n",
    "\n",
    "        query_tokens = tokenizer(recipe['text_en'][entity[0]:entity[1]], return_tensors='pt')['input_ids'].squeeze()\n",
    "        \n",
    "        if start_index_token < len(query_tokens):\n",
    "            # print(f'################# SKIPPING: START INDEX IS WITHIN QUERY #################\\nerrors: {error_count+1}\\nerror rate = {error_count/sample_count}')\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "        if re.match(r'mbert.*|bert-base-multilingual-cased.*', model_name.split('/')[-1]):\n",
    "            model_token_shift = 0\n",
    "        elif re.match(r'mdeberta.*', model_name.split('/')[-1]):\n",
    "            model_token_shift = 0\n",
    "\n",
    "        end_index_token = int(torch.argmax(end_scores)) + model_token_shift\n",
    "        decoded_input = ' '.join([f\"({index}, {tokenizer.decode([id])})\" for index, id in enumerate(input_ids)])\n",
    "        token_based_prediction = tokenizer.decode(input['input_ids'].squeeze()[start_index_token:end_index_token])\n",
    "        token_based_prediction_splitjoined = ''.join(token_based_prediction.split()).replace('#', '')\n",
    "\n",
    "        if end_index_token < start_index_token:\n",
    "            # print(f'################# SKIPPING: END INDEX TOKEN IS BEFORE START INDEX TOKEN #################\\nerrors: {error_count+1}\\nerror rate = {error_count/sample_count}')\n",
    "            error_count += 1\n",
    "            continue\n",
    "        \n",
    "        if start_index_token >= len(input_ids) - 1 or end_index_token > len(input_ids) - 1: # i put this here because the model sometimes predicts a token that is out of bounds\n",
    "            # print(f'################# SKIPPING: OUT-OF-BOUNDS PREDICTION #################\\nerrors: {error_count+1}\\nerror rate = {error_count/sample_count}')\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "        char_span_start = input.token_to_chars(start_index_token)\n",
    "        # the deberta tokenizer returns a space as well for words that are not at the start of the sentence, so here i am accounting for that, although not in an elegant way\n",
    "        # later on i will look into how to instantiate the tokenizer so that it does not return the space, although the devs say that that reduces performance\n",
    "        # so maybe that won't be a good idea\n",
    "        if search_span_context[char_span_start[0]:char_span_start[0]+1] == ' ':\n",
    "            char_span_start_adjusted = char_span_start[0] + 1 + prediction_shift\n",
    "        else:\n",
    "            char_span_start_adjusted = char_span_start[0] + prediction_shift\n",
    "        char_span_end = input.token_to_chars(end_index_token - 1)\n",
    "\n",
    "        if char_span_start is None or char_span_end is None:\n",
    "            # print(f'################# SKIPPING: CHAR SPAN IS NONE #################\\nerrors: {error_count+1}\\nerror rate = {error_count/sample_count}')\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "        char_span_end_adjusted = char_span_end[1] + prediction_shift\n",
    "        char_span_prediction = recipe['text_it'][char_span_start_adjusted:char_span_end_adjusted]\n",
    "        char_span_prediction_splitjoined = ''.join(char_span_prediction.split()).replace('#', '')\n",
    "        char_span = (char_span_start_adjusted, char_span_end_adjusted)\n",
    "        \n",
    "        if not char_span[0] <= char_span[1]:\n",
    "            # print(f'################# SKIPPING: CHAR SPAN END IS BEFORE CHAR SPAN START #################\\nerrors: {error_count+1}\\nerror rate = {error_count/sample_count}')\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "        error = False\n",
    "\n",
    "        if char_span_prediction_splitjoined != token_based_prediction_splitjoined:\n",
    "            print('ERROR: CHAR SPAN PREDICTION DOES NOT MATCH TOKEN-BASED PREDICTION')\n",
    "            # print(f'################# SKIPPING #################\\nerrors: {error_count+1}\\nerror rate = {error_count/sample_count}')\n",
    "            error = True\n",
    "            error_count += 1\n",
    "            print(f'recipe no.: {idx}, entity no.: {i}')\n",
    "            print(recipe['text_en'][entity[0]:entity[1]])\n",
    "            print(recipe['text_it'])\n",
    "            print('span_number:', span_number)\n",
    "            print('search_span:', search_span)\n",
    "            print('prediction_shift:', prediction_shift)\n",
    "            print('search_span_context:', search_span_context)\n",
    "            print('query_tokens:', query_tokens)\n",
    "            print('len(query_tokens):', len(query_tokens))\n",
    "            print('start_index_token:', start_index_token)\n",
    "            print('model_token_shift:', model_token_shift)\n",
    "            print('end_index_token:', end_index_token)\n",
    "            print('len(input_ids):', len(input_ids))\n",
    "            print('encoding:', input_ids)\n",
    "            print('decoded:', decoded_input)\n",
    "            print('prediction_tokens:', input['input_ids'].squeeze()[start_index_token:end_index_token])\n",
    "            print('token_based_prediction:', [token_based_prediction])\n",
    "            print('token_based_prediction_splitjoined:', [token_based_prediction_splitjoined])\n",
    "            print('gold:', [recipe['text_en'][entity[0]:entity[1]]])\n",
    "            print('char_span_start', char_span_start)\n",
    "            print('char_span_start_adjusted', char_span_start_adjusted)\n",
    "            print('char_span_end', char_span_end)\n",
    "            print('char_span_end_adjusted:', char_span_end_adjusted)\n",
    "            print('char_span_prediction:', [char_span_prediction])\n",
    "            print('char_span_prediction_splitjoined:', [char_span_prediction_splitjoined])\n",
    "            print('full context_en:', recipe['text_en'])\n",
    "            print('full context_it:', recipe['text_it'])\n",
    "            break\n",
    "        \n",
    "        recipe['entities_it'].append([char_span[0], char_span[1], recipe['entities_en'][i][2]])\n",
    "        inference_progress_bar.set_postfix({'errors': error_count, 'error rate': round(error_count/sample_count, 3)})\n",
    "\n",
    "        # prints go here\n",
    "        # print('----------------------------------------------')\n",
    "        # print(f'recipe no.: {idx}, entity no.: {i}')\n",
    "        # print(\"search_span_context[char_span_start[0]:char_span_end[1]]\", [search_span_context[char_span_start[0]:char_span_end[1]]])\n",
    "        # print(recipe['text_en'][entity[0]:entity[1]])\n",
    "        # print(recipe['text_it'])\n",
    "        # print('span_number:', span_number)\n",
    "        # print('search_span:', search_span)\n",
    "        # print('prediction_shift:', prediction_shift)\n",
    "        # print('search_span_context:', [search_span_context])\n",
    "        # print('query_tokens:', query_tokens)\n",
    "        # print('len(query_tokens):', len(query_tokens))\n",
    "        # print('start_index_token:', start_index_token)\n",
    "        # print('model_token_shift:', model_token_shift)\n",
    "        # print('end_index_token:', end_index_token)\n",
    "        # print('len(input_ids):', len(input_ids))\n",
    "        # print('char_span_start', char_span_start)\n",
    "        # print('char_span_start_adjusted', char_span_start_adjusted)\n",
    "        # print('char_span_end', char_span_end)\n",
    "        # print('char_span_end_adjusted:', char_span_end_adjusted)\n",
    "        # print('full context:', recipe['text_it'])\n",
    "        # print('encoding:', input_ids)\n",
    "        # print('decoded:', decoded_input)\n",
    "        # print('prediction_tokens:', input['input_ids'].squeeze()[start_index_token:end_index_token])\n",
    "        # print('char_span_prediction:', [char_span_prediction])\n",
    "        # print('char_span_prediction_splitjoined:', [char_span_prediction_splitjoined]) \n",
    "        # print('token_based_prediction:', [token_based_prediction])\n",
    "        # print('token_based_prediction_splitjoined:', [token_based_prediction_splitjoined])\n",
    "        # print('gold:', [recipe['text_en'][entity[0]:entity[1]]])\n",
    "        \n",
    "    if error:\n",
    "        break \n",
    "\n",
    "print('error_count:', error_count)\n",
    "print('sample_count:', sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recipe_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save aligned dataset to a new json\n",
    "import os\n",
    "json_dir = '/home/pgajo/working/food/data/TASTEset/data'\n",
    "json_filename_unformatted = f\"{bilingual_path.split('/')[-1].replace('.json', '')}_{model_name.split('/')[-1]}_{(error_count/sample_count):.4f}.json\".replace('unaligned', 'aligned')\n",
    "new_data = {'classes': data['classes'], 'annotations': recipe_list}\n",
    "with open(os.path.join(json_dir, json_filename_unformatted), 'w', encoding='utf8') as f:\n",
    "    json.dump(new_data, f, ensure_ascii=False)\n",
    "json_filename_unformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a format which can be used in Label Studio\n",
    "tasks = []\n",
    "for recipe in recipe_list:\n",
    "    predictions = []\n",
    "    results = []\n",
    "    languages = ['en', 'it']\n",
    "    entry = {}\n",
    "    for language in languages:\n",
    "        for entity in recipe[f'entities_{language}']:\n",
    "            results.append({\n",
    "                'from_name': f'label_{language}',\n",
    "                'to_name': f'text_{language}_ref',\n",
    "                'type': 'labels',\n",
    "                'value': {\n",
    "                    'start': entity[0],\n",
    "                    'end': entity[1],\n",
    "                    'labels': [entity[2]]}\n",
    "                    })\n",
    "        entry[f'text_{language}'] = recipe[f'text_{language}']\n",
    "    predictions.append({'model_version': model_name, 'result': results})\n",
    "    tasks.append({\n",
    "        'data': entry,\n",
    "        'predictions': predictions,\n",
    "    })\n",
    "\n",
    "# Save Label Studio tasks.json\n",
    "\n",
    "\n",
    "import os\n",
    "json_dir = '/home/pgajo/working/food/data/TASTEset/data'\n",
    "json_filename_ls = f\"{bilingual_path.split('/')[-1].replace('.json', '')}_{model_name.split('/')[-1]}_{(error_count/sample_count):.4f}_{'-'.join(languages)}_labelstudio.json\"\n",
    "with open(os.path.join(json_dir, json_filename_ls), 'w', encoding='utf8') as f:\n",
    "    json.dump(tasks, f, indent=4)\n",
    "print(f'Save {len(tasks)} tasks to \"{json_filename_ls}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
