{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/paolo-gajo/food.git\n",
    "!pip install datasets\n",
    "!pip install sentencepiece\n",
    "!pip install accelerate\n",
    "!pip install evaluate\n",
    "from google.colab import userdata\n",
    "token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./food\n",
    "import sys\n",
    "sys.path.append('./src/word_alignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgajo/working/food/food-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import DatasetDict\n",
    "from evaluate import load\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from utils import data_loader, SquadEvaluator, TASTEset, XLWADataset, save_local_model, push_card\n",
    "from datetime import datetime\n",
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "# model_name = 'microsoft/mdeberta-v3-base'\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "languages = [\n",
    "    # 'ru',\n",
    "    # 'nl',\n",
    "    'it',\n",
    "    # 'pt',\n",
    "    # 'et',\n",
    "    # 'es',\n",
    "    # 'hu',\n",
    "    # 'da',\n",
    "    # 'bg',\n",
    "    # 'sl',\n",
    "]\n",
    "\n",
    "lang_id = '-'.join(languages)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating samples (unshuffled)...: 100%|██████████| 200/200 [00:01<00:00, 184.75it/s]\n",
      "Parameter 'function'=<function TASTEset.__init__.<locals>.<lambda> at 0x7fdd2009c8b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 712/712 [00:00<00:00, 5115.66 examples/s]\n",
      "Map: 100%|██████████| 179/179 [00:00<00:00, 4583.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = f'/home/pgajo/working/food/data/EW-TASTE_en-it_DEEPL.json'\n",
    "results_path = f'/home/pgajo/working/food/results/tasteset/{lang_id}'\n",
    "data = TASTEset.from_json(\n",
    "        data_path,\n",
    "        tokenizer_name = model_name,\n",
    "        shuffle_languages=['it'],\n",
    "        src_lang = 'en',\n",
    "        dev_size = 0.2,\n",
    "        shuffled_size = 0,\n",
    "        unshuffled_size = 1,\n",
    "        # drop_duplicates = False,\n",
    "        # debug_dump = True,\n",
    "        n_rows=200,\n",
    "        )\n",
    "batch_size = 8\n",
    "dataset = data_loader(data,\n",
    "                    batch_size,\n",
    "                    )\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 ounces gin (Plymouth);0.5 ounces cointreau;0.5 ounces lillet blanc (or • Cocchi Americano •);3 dashes absinthe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.raw_data['train'][0]['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Embedding(119547, 512)\n",
      "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Embedding(tokenizer.vocab_size, 512),\n",
    "            # nn.MaxPool1d(3, 1),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, **batch):\n",
    "        batch = self.flatten(batch['input_ids'])\n",
    "        logits = self.linear_relu_stack(batch)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0503, -0.0801],\n",
      "         [ 0.0946, -0.0594],\n",
      "         [ 0.0005, -0.0655],\n",
      "         ...,\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056]],\n",
      "\n",
      "        [[ 0.0503, -0.0801],\n",
      "         [ 0.0251, -0.0750],\n",
      "         [ 0.0291, -0.0503],\n",
      "         ...,\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056]],\n",
      "\n",
      "        [[ 0.0503, -0.0801],\n",
      "         [-0.0083, -0.2299],\n",
      "         [ 0.1184, -0.1218],\n",
      "         ...,\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0503, -0.0801],\n",
      "         [ 0.0598, -0.1037],\n",
      "         [ 0.1109, -0.1295],\n",
      "         ...,\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056]],\n",
      "\n",
      "        [[ 0.0503, -0.0801],\n",
      "         [ 0.0946, -0.0594],\n",
      "         [-0.0312, -0.1064],\n",
      "         ...,\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056]],\n",
      "\n",
      "        [[ 0.0503, -0.0801],\n",
      "         [ 0.0955,  0.0050],\n",
      "         [ 0.1109, -0.1295],\n",
      "         ...,\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056],\n",
      "         [ 0.0511, -0.1056]]], device='cuda:0', grad_fn=<ViewBackward0>) torch.Size([8, 262, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset['train']:\n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device)\n",
    "    outputs = model(**batch)\n",
    "    print(outputs, outputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 262])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(119547, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(tokenizer.vocab_size,512)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101, 61694, 10133, 15127, 11324, 10124, 10931, 19139,   102])\n",
      "['[CLS]', 'hell', '##o', 'my', 'name', 'is', 'pa', '##olo', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer('hello my name is paolo', return_tensors='pt')['input_ids'].squeeze()\n",
    "print(tokens)\n",
    "print([tokenizer.decode(token) for token in tokens])\n",
    "embedding(tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-5\n",
    "eps = 1e-8\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=lr,\n",
    "                            eps=eps\n",
    "                            )\n",
    "\n",
    "evaluator = SquadEvaluator(tokenizer,\n",
    "                        model,\n",
    "                        load(\"squad_v2\"),\n",
    "                        )\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    epoch_train_loss = 0\n",
    "    model.train()\n",
    "    split = 'train'\n",
    "    progbar = tqdm(enumerate(dataset[split]),\n",
    "                            total=len(dataset[split]),\n",
    "                            desc=f\"{split} - epoch {epoch + 1}\")\n",
    "    for i, batch in progbar:\n",
    "        for key in batch.keys():\n",
    "            batch[key].to(device)\n",
    "        outputs = model(**batch) # ['loss', 'start_logits', 'end_logits']\n",
    "        loss = outputs[0].mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        loss_tmp = round(epoch_train_loss / (i + 1), 4)\n",
    "        progbar.set_postfix({'Loss': loss_tmp})\n",
    "        \n",
    "        evaluator.get_eval_batch(outputs, batch, split)\n",
    "\n",
    "    evaluator.evaluate(model, split, epoch)\n",
    "    epoch_train_loss /= len(dataset[split])\n",
    "    evaluator.epoch_metrics[f'{split}_loss'] = epoch_train_loss\n",
    "\n",
    "    evaluator.print_metrics(current_epoch = epoch, current_split = split)\n",
    "\n",
    "    # eval on dev\n",
    "    epoch_dev_loss = 0\n",
    "    model.eval()\n",
    "    split = 'dev'\n",
    "    progbar = tqdm(enumerate(dataset[split]),\n",
    "                            total=len(dataset[split]),\n",
    "                            desc=f\"{split} - epoch {epoch + 1}\")\n",
    "    for i, batch in progbar:\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs[0].mean()\n",
    "        epoch_dev_loss += loss.item()\n",
    "        loss_tmp = round(epoch_dev_loss / (i + 1), 4)\n",
    "        progbar.set_postfix({'Loss': loss_tmp})\n",
    "        \n",
    "        evaluator.get_eval_batch(outputs, batch, split)\n",
    "    \n",
    "    evaluator.evaluate(model, split, epoch)\n",
    "    epoch_dev_loss /= len(dataset[split])\n",
    "    evaluator.epoch_metrics[f'{split}_loss'] = epoch_dev_loss\n",
    "\n",
    "    evaluator.print_metrics(current_epoch = epoch, current_split = split)\n",
    "\n",
    "    evaluator.store_metrics()\n",
    "\n",
    "    if evaluator.stop_training:\n",
    "        print(f'Early stopping triggered on epoch {epoch}. \\\n",
    "            \\nBest epoch: {evaluator.epoch_best}.')                                               \n",
    "        break\n",
    "\n",
    "evaluator.print_metrics()\n",
    "if not os.path.isdir(results_path):\n",
    "    os.makedirs(results_path)\n",
    "evaluator.save_metrics_to_csv(results_path)\n",
    "\n",
    "model_dict = {\n",
    "    'bert-base-multilingual-cased': 'mbert',\n",
    "    'microsoft/mdeberta-v3-base': 'mdeberta',\n",
    "}\n",
    "\n",
    "if not hasattr(data, 'unshuffled_size'):\n",
    "    data.unshuffled_size = 1\n",
    "if not hasattr(data, 'shuffled_size'):\n",
    "    data.shuffled_size = 0\n",
    "    \n",
    "# model save folder\n",
    "model_dir = './models'\n",
    "save_name = f\"{model_dict[model_name]}_{data.name}_U{data.unshuffled_size}_S{data.shuffled_size}_E{evaluator.epoch_best}_DEV{str(round(evaluator.exact_dev_best, ndigits=0))}_DROP{str(int(data.drop_duplicates))}\"\n",
    "save_name = save_name + \"_test\" # comment if not testing\n",
    "model_save_dir = os.path.join(model_dir, f\"{data.name}/{save_name}\")\n",
    "if not os.path.isdir(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "evaluator.save_metrics_to_csv(os.path.join(model_save_dir, 'metrics'))\n",
    "save_local_model(model_save_dir, model, tokenizer)\n",
    "\n",
    "repo_id = f\"pgajo/{save_name}\"\n",
    "print('repo_id', repo_id)\n",
    "api = HfApi(token = os.environ['HF_TOKEN'])\n",
    "api.create_repo(repo_id)\n",
    "df_desc = pd.DataFrame(evaluator.metrics).round(2)\n",
    "df_desc.index += 1\n",
    "df_desc.index.name = 'epoch'\n",
    "df_desc = df_desc.to_markdown()\n",
    "model_description = f'''\n",
    "Model: {model_dict[model_name]}\\n\n",
    "Dataset: {data.name}\\n\n",
    "Unshuffled ratio: {data.unshuffled_size}\\n\n",
    "Shuffled ratio: {data.shuffled_size}\\n\n",
    "Best exact match epoch: {evaluator.epoch_best}\\n\n",
    "Best exact match: {str(round(evaluator.exact_dev_best, ndigits=2))}\\n\n",
    "Drop duplicates: {data.drop_duplicates}\\n\n",
    "Optimizer lr = {lr}\\n\n",
    "Optimizer eps = {eps}\\n\n",
    "Batch size = {batch_size}\\n\n",
    "Metrics:\\n\n",
    "{df_desc}\n",
    "'''\n",
    "push_card(repo_id=repo_id,\n",
    "        model_name=model_name,\n",
    "        model_description=model_description,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.upload_folder(repo_id=repo_id, folder_path=model_save_dir) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
