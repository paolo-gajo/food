{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/pgajo/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# login to huggingface and push model to hub\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the TASTEset aligner from the item-wise translated TASTEset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final shape of the dataset:\n",
    "# DatasetDict({\n",
    "#     train: Dataset({\n",
    "#         features: ['id_sentence', 'id_alignment', 'query', 'context', 'answer', 'answer_start', 'answer_end', 'answer_start_token', 'answer_end_token', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#         num_rows: 37245\n",
    "#     })\n",
    "#     validation: Dataset({\n",
    "#         features: ['id_sentence', 'id_alignment', 'query', 'context', 'answer', 'answer_start', 'answer_end', 'answer_start_token', 'answer_end_token', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#         num_rows: 3941\n",
    "#     })\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('/home/pgajo/working/food/data/TASTEset/data/TASTEset_updated_en-it_itemwise.json') as f:\n",
    "    data = json.load(f)\n",
    "recipe_list = data['annotations']\n",
    "print(len(recipe_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_en': '5 ounces rum 4 ounces triple sec 3 ounces Tia Maria 20 ounces orange juice',\n",
       " 'entities_en': [[0, 1, 'QUANTITY'],\n",
       "  [2, 8, 'UNIT'],\n",
       "  [9, 12, 'FOOD'],\n",
       "  [13, 14, 'QUANTITY'],\n",
       "  [15, 21, 'UNIT'],\n",
       "  [22, 32, 'FOOD'],\n",
       "  [33, 34, 'QUANTITY'],\n",
       "  [35, 41, 'UNIT'],\n",
       "  [42, 51, 'FOOD'],\n",
       "  [52, 54, 'QUANTITY'],\n",
       "  [55, 61, 'UNIT'],\n",
       "  [62, 74, 'FOOD']],\n",
       " 'text_it': \"5 once rum 4 once triple sec 3 once Tia Maria 20 once succo d'arancia\",\n",
       " 'entities_it': [[0, 1, 'QUANTITY'],\n",
       "  [2, 6, 'UNIT'],\n",
       "  [7, 10, 'FOOD'],\n",
       "  [11, 12, 'QUANTITY'],\n",
       "  [13, 17, 'UNIT'],\n",
       "  [18, 28, 'FOOD'],\n",
       "  [29, 30, 'QUANTITY'],\n",
       "  [31, 35, 'UNIT'],\n",
       "  [36, 45, 'FOOD'],\n",
       "  [46, 48, 'QUANTITY'],\n",
       "  [49, 53, 'UNIT'],\n",
       "  [54, 69, 'FOOD']]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgajo/working/food/food-env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token_id</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>81</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>93</td>\n",
       "      <td>94</td>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>105</td>\n",
       "      <td>106</td>\n",
       "      <td>107</td>\n",
       "      <td>108</td>\n",
       "      <td>109</td>\n",
       "      <td>110</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>113</td>\n",
       "      <td>114</td>\n",
       "      <td>115</td>\n",
       "      <td>116</td>\n",
       "      <td>117</td>\n",
       "      <td>118</td>\n",
       "      <td>119</td>\n",
       "      <td>120</td>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "      <td>125</td>\n",
       "      <td>126</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>tensor(451)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(117417)</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(476)</td>\n",
       "      <td>tensor(35145)</td>\n",
       "      <td>tensor(282)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(50160)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(175099)</td>\n",
       "      <td>tensor(264)</td>\n",
       "      <td>tensor(4200)</td>\n",
       "      <td>tensor(265)</td>\n",
       "      <td>tensor(107664)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(9111)</td>\n",
       "      <td>tensor(261)</td>\n",
       "      <td>tensor(262)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(131880)</td>\n",
       "      <td>tensor(19304)</td>\n",
       "      <td>tensor(286)</td>\n",
       "      <td>tensor(272)</td>\n",
       "      <td>tensor(800)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(117353)</td>\n",
       "      <td>tensor(264)</td>\n",
       "      <td>tensor(367)</td>\n",
       "      <td>tensor(116511)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(144263)</td>\n",
       "      <td>tensor(2179)</td>\n",
       "      <td>tensor(1517)</td>\n",
       "      <td>tensor(31943)</td>\n",
       "      <td>tensor(550)</td>\n",
       "      <td>tensor(45053)</td>\n",
       "      <td>tensor(1065)</td>\n",
       "      <td>tensor(7604)</td>\n",
       "      <td>tensor(264)</td>\n",
       "      <td>tensor(263)</td>\n",
       "      <td>tensor(334)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(138361)</td>\n",
       "      <td>tensor(275)</td>\n",
       "      <td>tensor(7239)</td>\n",
       "      <td>tensor(41896)</td>\n",
       "      <td>tensor(29587)</td>\n",
       "      <td>tensor(265)</td>\n",
       "      <td>tensor(358)</td>\n",
       "      <td>tensor(265)</td>\n",
       "      <td>tensor(123624)</td>\n",
       "      <td>tensor(662)</td>\n",
       "      <td>tensor(1161)</td>\n",
       "      <td>tensor(89880)</td>\n",
       "      <td>tensor(395)</td>\n",
       "      <td>tensor(738)</td>\n",
       "      <td>tensor(142762)</td>\n",
       "      <td>tensor(10233)</td>\n",
       "      <td>tensor(357)</td>\n",
       "      <td>tensor(4700)</td>\n",
       "      <td>tensor(82378)</td>\n",
       "      <td>tensor(264)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(138110)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(43217)</td>\n",
       "      <td>tensor(334)</td>\n",
       "      <td>tensor(18985)</td>\n",
       "      <td>tensor(116394)</td>\n",
       "      <td>tensor(114503)</td>\n",
       "      <td>tensor(8038)</td>\n",
       "      <td>tensor(180812)</td>\n",
       "      <td>tensor(23330)</td>\n",
       "      <td>tensor(275)</td>\n",
       "      <td>tensor(724)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(129803)</td>\n",
       "      <td>tensor(104251)</td>\n",
       "      <td>tensor(278)</td>\n",
       "      <td>tensor(400)</td>\n",
       "      <td>tensor(260)</td>\n",
       "      <td>tensor(198760)</td>\n",
       "      <td>tensor(18077)</td>\n",
       "      <td>tensor(272)</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_str</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>con</td>\n",
       "      <td></td>\n",
       "      <td>osso</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>10</td>\n",
       "      <td>bone</td>\n",
       "      <td>in</td>\n",
       "      <td></td>\n",
       "      <td>chicken</td>\n",
       "      <td></td>\n",
       "      <td>thigh</td>\n",
       "      <td>s</td>\n",
       "      <td>(2</td>\n",
       "      <td>-</td>\n",
       "      <td>3/4</td>\n",
       "      <td></td>\n",
       "      <td>lb</td>\n",
       "      <td>.</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>skins</td>\n",
       "      <td>remove</td>\n",
       "      <td>d</td>\n",
       "      <td>)</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td>ounce</td>\n",
       "      <td>s</td>\n",
       "      <td>T</td>\n",
       "      <td>ACO</td>\n",
       "      <td></td>\n",
       "      <td>BELL</td>\n",
       "      <td>®</td>\n",
       "      <td>Th</td>\n",
       "      <td>ick</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>Chun</td>\n",
       "      <td>ky</td>\n",
       "      <td>Sal</td>\n",
       "      <td>s</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>onion</td>\n",
       "      <td>(</td>\n",
       "      <td>cho</td>\n",
       "      <td>pped</td>\n",
       "      <td>Save</td>\n",
       "      <td>-</td>\n",
       "      <td>A</td>\n",
       "      <td>-</td>\n",
       "      <td>Lot</td>\n",
       "      <td>—</td>\n",
       "      <td>$</td>\n",
       "      <td>0.99</td>\n",
       "      <td>th</td>\n",
       "      <td>ru</td>\n",
       "      <td>01/2</td>\n",
       "      <td>3)</td>\n",
       "      <td>2</td>\n",
       "      <td>table</td>\n",
       "      <td>spoon</td>\n",
       "      <td>s</td>\n",
       "      <td></td>\n",
       "      <td>curry</td>\n",
       "      <td></td>\n",
       "      <td>powder</td>\n",
       "      <td>1</td>\n",
       "      <td>cup</td>\n",
       "      <td>Knu</td>\n",
       "      <td>dsen</td>\n",
       "      <td>Light</td>\n",
       "      <td>Sour</td>\n",
       "      <td>Cream</td>\n",
       "      <td>(</td>\n",
       "      <td>or</td>\n",
       "      <td></td>\n",
       "      <td>BREAK</td>\n",
       "      <td>STONE</td>\n",
       "      <td>'</td>\n",
       "      <td>S</td>\n",
       "      <td></td>\n",
       "      <td>Reduced</td>\n",
       "      <td>Fat</td>\n",
       "      <td>)</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2               3          4    \\\n",
       "token_id           0            1            2               3          4   \n",
       "token      tensor(1)  tensor(451)  tensor(260)  tensor(117417)  tensor(2)   \n",
       "token_str      [CLS]          con                         osso      [SEP]   \n",
       "\n",
       "                   5              6            7            8    \\\n",
       "token_id             5              6            7            8   \n",
       "token      tensor(476)  tensor(35145)  tensor(282)  tensor(260)   \n",
       "token_str           10           bone           in                \n",
       "\n",
       "                     9            10              11           12   \\\n",
       "token_id               9           10              11           12   \n",
       "token      tensor(50160)  tensor(260)  tensor(175099)  tensor(264)   \n",
       "token_str        chicken                        thigh            s   \n",
       "\n",
       "                    13           14              15           16   \\\n",
       "token_id             13           14              15           16   \n",
       "token      tensor(4200)  tensor(265)  tensor(107664)  tensor(260)   \n",
       "token_str            (2            -             3/4                \n",
       "\n",
       "                    17           18           19           20   \\\n",
       "token_id             17           18           19           20   \n",
       "token      tensor(9111)  tensor(261)  tensor(262)  tensor(260)   \n",
       "token_str            lb            .            ,                \n",
       "\n",
       "                      21             22           23           24   \\\n",
       "token_id               21             22           23           24   \n",
       "token      tensor(131880)  tensor(19304)  tensor(286)  tensor(272)   \n",
       "token_str           skins         remove            d            )   \n",
       "\n",
       "                   25           26              27           28           29   \\\n",
       "token_id            25           26              27           28           29   \n",
       "token      tensor(800)  tensor(260)  tensor(117353)  tensor(264)  tensor(367)   \n",
       "token_str           16                        ounce            s            T   \n",
       "\n",
       "                      30           31              32            33   \\\n",
       "token_id               30           31              32            33   \n",
       "token      tensor(116511)  tensor(260)  tensor(144263)  tensor(2179)   \n",
       "token_str             ACO                         BELL             ®   \n",
       "\n",
       "                    34             35           36             37   \\\n",
       "token_id             34             35           36             37   \n",
       "token      tensor(1517)  tensor(31943)  tensor(550)  tensor(45053)   \n",
       "token_str            Th            ick            &           Chun   \n",
       "\n",
       "                    38            39           40           41           42   \\\n",
       "token_id             38            39           40           41           42   \n",
       "token      tensor(1065)  tensor(7604)  tensor(264)  tensor(263)  tensor(334)   \n",
       "token_str            ky           Sal            s            a            1   \n",
       "\n",
       "                   43              44           45            46   \\\n",
       "token_id            43              44           45            46   \n",
       "token      tensor(260)  tensor(138361)  tensor(275)  tensor(7239)   \n",
       "token_str                        onion            (           cho   \n",
       "\n",
       "                     47             48           49           50   \\\n",
       "token_id              47             48           49           50   \n",
       "token      tensor(41896)  tensor(29587)  tensor(265)  tensor(358)   \n",
       "token_str           pped           Save            -            A   \n",
       "\n",
       "                   51              52           53            54   \\\n",
       "token_id            51              52           53            54   \n",
       "token      tensor(265)  tensor(123624)  tensor(662)  tensor(1161)   \n",
       "token_str            -             Lot            —             $   \n",
       "\n",
       "                     55           56           57              58   \\\n",
       "token_id              55           56           57              58   \n",
       "token      tensor(89880)  tensor(395)  tensor(738)  tensor(142762)   \n",
       "token_str           0.99           th           ru            01/2   \n",
       "\n",
       "                     59           60            61             62   \\\n",
       "token_id              59           60            61             62   \n",
       "token      tensor(10233)  tensor(357)  tensor(4700)  tensor(82378)   \n",
       "token_str             3)            2         table          spoon   \n",
       "\n",
       "                   63           64              65           66   \\\n",
       "token_id            63           64              65           66   \n",
       "token      tensor(264)  tensor(260)  tensor(138110)  tensor(260)   \n",
       "token_str            s                        curry                \n",
       "\n",
       "                     67           68             69              70   \\\n",
       "token_id              67           68             69              70   \n",
       "token      tensor(43217)  tensor(334)  tensor(18985)  tensor(116394)   \n",
       "token_str         powder            1            cup             Knu   \n",
       "\n",
       "                      71            72              73             74   \\\n",
       "token_id               71            72              73             74   \n",
       "token      tensor(114503)  tensor(8038)  tensor(180812)  tensor(23330)   \n",
       "token_str            dsen         Light            Sour          Cream   \n",
       "\n",
       "                   75           76           77              78   \\\n",
       "token_id            75           76           77              78   \n",
       "token      tensor(275)  tensor(724)  tensor(260)  tensor(129803)   \n",
       "token_str            (           or                        BREAK   \n",
       "\n",
       "                      79           80           81           82   \\\n",
       "token_id               79           80           81           82   \n",
       "token      tensor(104251)  tensor(278)  tensor(400)  tensor(260)   \n",
       "token_str           STONE            '            S                \n",
       "\n",
       "                      83             84           85         86         87   \\\n",
       "token_id               83             84           85         86         87   \n",
       "token      tensor(198760)  tensor(18077)  tensor(272)  tensor(2)  tensor(0)   \n",
       "token_str         Reduced            Fat            )      [SEP]      [PAD]   \n",
       "\n",
       "                 88         89         90         91         92         93   \\\n",
       "token_id          88         89         90         91         92         93   \n",
       "token      tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)   \n",
       "token_str      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]   \n",
       "\n",
       "                 94         95         96         97         98         99   \\\n",
       "token_id          94         95         96         97         98         99   \n",
       "token      tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)   \n",
       "token_str      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]   \n",
       "\n",
       "                 100        101        102        103        104        105  \\\n",
       "token_id         100        101        102        103        104        105   \n",
       "token      tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)   \n",
       "token_str      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]   \n",
       "\n",
       "                 106        107        108        109        110        111  \\\n",
       "token_id         106        107        108        109        110        111   \n",
       "token      tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)   \n",
       "token_str      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]   \n",
       "\n",
       "                 112        113        114        115        116        117  \\\n",
       "token_id         112        113        114        115        116        117   \n",
       "token      tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)   \n",
       "token_str      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]   \n",
       "\n",
       "                 118        119        120        121        122        123  \\\n",
       "token_id         118        119        120        121        122        123   \n",
       "token      tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)   \n",
       "token_str      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]      [PAD]   \n",
       "\n",
       "                 124        125        126        127  \n",
       "token_id         124        125        126        127  \n",
       "token      tensor(0)  tensor(0)  tensor(0)  tensor(0)  \n",
       "token_str      [PAD]      [PAD]      [PAD]      [PAD]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_name = 'microsoft/mdeberta-v3-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "padding = 128\n",
    "example_context = \"10 bone in chicken thighs (2-3/4 lb., skins removed) 16 ounces TACO BELL® Thick & Chunky Salsa 1 onion (chopped Save-A-Lot — $0.99 thru 01/23) 2 tablespoons curry powder 1 cup Knudsen Light Sour Cream (or BREAKSTONE'S Reduced Fat)\"\n",
    "example_query = \"con osso\"\n",
    "input = tokenizer(example_query, example_context, padding='max_length', truncation=True, max_length=padding, return_tensors=\"pt\")\n",
    "tokenizer.decode(input['input_ids'].squeeze())\n",
    "example = []\n",
    "for i, token in enumerate(input['input_ids'].squeeze()):\n",
    "    example.append([i, token, tokenizer.decode(token)])\n",
    "import pandas as pd\n",
    "example_df = pd.DataFrame(example, columns=['token_id', 'token', 'token_str'])\n",
    "# transpose example_df\n",
    "pd.options.display.max_columns = 128\n",
    "example_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 maximum encoding length: 128\n"
     ]
    }
   ],
   "source": [
    "dataset_list = []\n",
    "encoding_len = 0\n",
    "for i, recipe in enumerate(recipe_list):\n",
    "    for j, entity in enumerate(recipe['entities_en']):\n",
    "        entry = {}\n",
    "        entry['id_recipe'] = i\n",
    "        entry['id_entity'] = j\n",
    "        entry['query'] = recipe['text_en'][entity[0]:entity[1]]\n",
    "        entry['context'] = recipe['text_it']\n",
    "        entry['answer'] = recipe['text_it'][recipe['entities_it'][j][0]:recipe['entities_it'][j][1]]\n",
    "        entry['answer_start'] = recipe['entities_it'][j][0]\n",
    "        entry['answer_end'] = recipe['entities_it'][j][1]\n",
    "        char_check = entry['context'][entry['answer_start']:entry['answer_end']]\n",
    "        query_encoding = tokenizer(entry['query'])\n",
    "        context_encoding = tokenizer(entry['context'])\n",
    "        shift_type = {\n",
    "            'bert': (-1, 0),\n",
    "            'roberta': (0, 1),\n",
    "        }\n",
    "        shifts = shift_type['bert']\n",
    "        entry['answer_start_token'] = context_encoding.char_to_token(entry['answer_start']) + len(query_encoding['input_ids']) + shifts[0]\n",
    "        entry['answer_end_token'] = context_encoding.char_to_token(entry['answer_end']-1) + len(query_encoding['input_ids']) + shifts[1]\n",
    "\n",
    "        input_encoding = tokenizer(entry['query'], entry['context'],\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=padding,\n",
    "                                   )\n",
    "        # print(input_encoding['input_ids'])\n",
    "        # print(tokenizer.decode(input_encoding['input_ids']))\n",
    "        \n",
    "        # input_encoding_nopadding = tokenizer(entry['query'], entry['context'])\n",
    "        # if len(input_encoding_nopadding) > 0.5*padding:\n",
    "        #     print(tokenizer.decode(input_encoding['input_ids']))\n",
    "        if encoding_len < len(input_encoding['input_ids']):\n",
    "            encoding_len = len(input_encoding['input_ids'])\n",
    "            max_len_id = i\n",
    "        token_check = tokenizer.decode(input_encoding['input_ids'][entry['answer_start_token']:entry['answer_end_token']])\n",
    "\n",
    "        if ''.join(char_check.split()) != ''.join(token_check.split()):\n",
    "            print('ERROR: char_check != token_check')\n",
    "            print(\"entry['id_recipe']\", entry['id_recipe'])\n",
    "            print(\"entry['id_entity']\", entry['id_entity'])\n",
    "            print(\"entry['query']\", entry['query'])\n",
    "            print(\"entry['context']\", entry['context'])\n",
    "            print(\"char_check\", char_check)\n",
    "            print(\"token_check\", token_check)\n",
    "            print(\"entry['answer_start_token']\", entry['answer_start_token'])\n",
    "            print(\"entry['answer_end_token']\", entry['answer_end_token'])\n",
    "            print(\"entry['answer_start']\", entry['answer_start'])\n",
    "            print(\"entry['answer_end']\", entry['answer_end'])\n",
    "            print('-------------------------')\n",
    "            continue\n",
    "        dataset_list.append(entry)\n",
    "print(max_len_id, 'maximum encoding length:', encoding_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0321aa87faf04065b34f3d2b4d3ed800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10689 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b79270b395947f09cc255b884f8cf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id_recipe', 'id_entity', 'query', 'context', 'answer', 'answer_start', 'answer_end', 'answer_start_token', 'answer_end_token', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 10689\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id_recipe', 'id_entity', 'query', 'context', 'answer', 'answer_start', 'answer_end', 'answer_start_token', 'answer_end_token', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2673\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "dataset_unsplit = Dataset.from_list(dataset_list)\n",
    "\n",
    "train_test_split = dataset_unsplit.train_test_split(test_size=0.2)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test']\n",
    "})\n",
    "\n",
    "# print(dataset)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['query'], example['context'], padding='max_length', truncation=True, max_length=padding)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids',\n",
    "                                               'token_type_ids',\n",
    "                                                'attention_mask',\n",
    "                                                'answer_start_token',\n",
    "                                                'answer_end_token'])\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_loader = torch.utils.data.DataLoader(tokenized_dataset['train'], batch_size = 16, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(tokenized_dataset['validation'], batch_size = 16, shuffle = True)\n",
    "\n",
    "# inspect the first batch\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     print(batch['input_ids'])\n",
    "#     print(batch['token_type_ids'])\n",
    "#     print(batch['attention_mask'])\n",
    "#     print(batch['answer_start_token'])\n",
    "#     print(batch['answer_end_token'])\n",
    "#     print(batch.keys())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=1000)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_name = \"pgajo/mdeberta-v3-base-xl-wa\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "parallel_model = torch.nn.DataParallel(model)  # Use DataParallel\n",
    "optimizer = torch.optim.AdamW(parallel_model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############Train############\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c08a3d44da497e8c998da6545fc23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgajo/working/food/food-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 / 669 Loss: 0.4\n",
      "Batch 200 / 669 Loss: 0.4\n",
      "Batch 300 / 669 Loss: 0.6\n",
      "Batch 400 / 669 Loss: 0.5\n",
      "Batch 500 / 669 Loss: 0.6\n",
      "Batch 600 / 669 Loss: 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade56e44de56424ca3ba5a5bf016990a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 / 168 Loss: 0.4\n",
      "\n",
      "-------Epoch  1 -------\n",
      "Training Loss: 0.44667230529229673 \n",
      "Validation Loss: 0.30893945869147066 \n",
      "Time:  398.33397102355957 \n",
      "----------------------- \n",
      "\n",
      "\n",
      "############Train############\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a2ae27311b4b31b327f699bf12711a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 / 669 Loss: 0.2\n",
      "Batch 200 / 669 Loss: 0.3\n",
      "Batch 300 / 669 Loss: 0.3\n",
      "Batch 400 / 669 Loss: 0.6\n",
      "Batch 500 / 669 Loss: 0.4\n",
      "Batch 600 / 669 Loss: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700fc400d02b4af792898080e3be79c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 / 168 Loss: 0.3\n",
      "\n",
      "-------Epoch  2 -------\n",
      "Training Loss: 0.3307734220802035 \n",
      "Validation Loss: 0.2938616190190909 \n",
      "Time:  398.6046943664551 \n",
      "----------------------- \n",
      "\n",
      "\n",
      "############Train############\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6da7b0cb8f24636940218168848b4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 / 669 Loss: 0.4\n",
      "Batch 200 / 669 Loss: 0.2\n",
      "Batch 300 / 669 Loss: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "current_timeanddate = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# save validation statistics to a dataframe\n",
    "df = pd.DataFrame()\n",
    "# append to csv\n",
    "df.to_csv(f\"{current_timeanddate}_{model_name.split('/')[-1]}_train_val_loss.csv\")\n",
    "\n",
    "epochs = 3\n",
    "whole_train_eval_time = time.time()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    # Set parallel model in train mode\n",
    "    parallel_model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    print(\"############Train############\")\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)): \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": batch['input_ids'].to(device),\n",
    "            'token_type_ids': batch['token_type_ids'].to(device),\n",
    "            \"attention_mask\": batch['attention_mask'].to(device),\n",
    "            \"start_positions\": batch['answer_start_token'].to(device),\n",
    "            \"end_positions\": batch['answer_end_token'].to(device),\n",
    "        }\n",
    "\n",
    "        outputs = parallel_model(**inputs)\n",
    "        loss = outputs[0].mean()\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            print(\"Batch {:} / {:}\".format(batch_idx + 1, len(train_loader)), \"Loss:\", round(loss.item(), 1))\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    ########## Evaluation ##################\n",
    "    parallel_model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(val_loader), total=len(val_loader)): \n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch['input_ids'].to(device),\n",
    "                'token_type_ids': batch['token_type_ids'].to(device),\n",
    "                \"attention_mask\": batch['attention_mask'].to(device),\n",
    "                \"start_positions\": batch['answer_start_token'].to(device),\n",
    "                \"end_positions\": batch['answer_end_token'].to(device),\n",
    "            }\n",
    "            \n",
    "            outputs = parallel_model(**inputs)\n",
    "            loss = outputs[0].mean()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % print_every == 0:\n",
    "                print(\"Batch {:} / {:}\".format(batch_idx + 1, len(val_loader)), \"Loss:\", round(loss.item(), 1))\n",
    "\n",
    "    epoch_loss /= len(val_loader)\n",
    "    val_losses.append(epoch_loss)\n",
    "\n",
    "    print(\"\\n-------Epoch \", epoch + 1, \n",
    "          \"-------\"\n",
    "          \"\\nTraining Loss:\", train_losses[-1],\n",
    "          \"\\nValidation Loss:\", val_losses[-1],\n",
    "          \"\\nTime: \", (time.time() - epoch_time),\n",
    "          \"\\n-----------------------\",\n",
    "          \"\\n\\n\")\n",
    "    \n",
    "    # save epoch number, train loss and validation loss to a dataframe which has columns: epoch, train_loss, val_loss\n",
    "    df = pd.DataFrame({'epoch': range(epoch+1), 'train_loss': train_losses, 'val_loss': val_losses})\n",
    "    # append to csv\n",
    "    df.to_csv(f\"{current_timeanddate}_{model_name.split('/')[-1]}_train_val_loss.csv\")\n",
    "\n",
    "print(\"Total training and evaluation time: \", (time.time() - whole_train_eval_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"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\")\n",
    "model.push_to_hub(f\"pgajo/mdebertav3-tasteset-recipe-aligner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami, create_repo, ModelCard, ModelCardData\n",
    "\n",
    "user = whoami()['pgajo']\n",
    "repo_id = f\"{user}/{model_name.split('/')[-1]}\"\n",
    "url = create_repo(repo_id, exist_ok=True)\n",
    "card_data = ModelCardData(language='en', license='mit', library_name='pytorch')\n",
    "card = ModelCard.from_template(\n",
    "    card_data,\n",
    "    model_id=model_name.split('/')[-1],\n",
    "    model_description=\"Recipe aligner model trained on machine-translated TASTEset recipes\",\n",
    "    developers=\"Paolo Gajo\",\n",
    "    repo=\"https://github.com/huggingface/huggingface_hub\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "# parallel_model = torch.nn.DataParallel(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr = 2e-5)\n",
    "# training time\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "epochs = 3\n",
    "whole_train_eval_time = time.time()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  epoch_time = time.time()\n",
    "\n",
    "  # Set model in train mode\n",
    "  model.train()\n",
    "    \n",
    "  epoch_loss = 0\n",
    "\n",
    "  print(\"############Train############\")\n",
    "\n",
    "  for batch_idx,batch in tqdm(enumerate(train_loader), total=len(train_loader)): \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    inputs = {\n",
    "      \"input_ids\": batch['input_ids'].to(device),\n",
    "      'token_type_ids': batch['token_type_ids'].to(device),\n",
    "      \"attention_mask\": batch['attention_mask'].to(device),\n",
    "      \"start_positions\": batch['answer_start_token'].to(device),\n",
    "      \"end_positions\": batch['answer_end_token'].to(device),\n",
    "    }\n",
    "    # print(inputs)\n",
    "    outputs = model(**inputs)\n",
    "    # print(outputs.keys())\n",
    "\n",
    "    # start_logits, end_logits = outputs[1], outputs[2]\n",
    "    # for j in range(len(start_logits)):\n",
    "    #     start_prediction = torch.argmax(start_logits[j])\n",
    "    #     end_prediction = torch.argmax(end_logits[j])  # Add 1 to include the end token\n",
    "    #     start_gold = batch['answer_start_token'][j]\n",
    "    #     end_gold = batch['answer_end_token'][j]\n",
    "    #     answer = tokenizer.decode(batch['input_ids'][j][start_prediction:end_prediction])\n",
    "    #     gold = tokenizer.decode(batch['input_ids'][j][start_gold:end_gold])\n",
    "    #     print(f\"Input: {tokenizer.decode(batch['input_ids'][j])}\")\n",
    "    #     print(f\"Prediction: {answer}\")\n",
    "    #     print(f\"Start prediction: {start_prediction}\")\n",
    "    #     print(f\"End prediction: {end_prediction}\")\n",
    "    #     print(f\"Gold: {gold}\")\n",
    "    #     print('-------------------------------------------')\n",
    "\n",
    "    loss = outputs[0].mean()\n",
    "    # print(f\"Loss: {loss.item()}\")\n",
    "    epoch_loss += loss.item()\n",
    "    # do a backwards pass \n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    # Find the total loss\n",
    "    \n",
    "\n",
    "    if (batch_idx+1) % print_every == 0:\n",
    "      print(\"Batch {:} / {:}\".format(batch_idx+1,len(train_loader)),\"\\nLoss:\", round(loss.item(),1),\"\\n\")\n",
    "\n",
    "  epoch_loss /= len(train_loader)\n",
    "  train_losses.append(epoch_loss)\n",
    "\n",
    "  ##########Evaluation##################\n",
    "\n",
    "  # Set model in evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  print(\"############Evaluate############\")\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch_idx,batch in tqdm(enumerate(val_loader), total=len(val_loader)): \n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "      inputs = {\n",
    "        \"input_ids\": batch['input_ids'].to(device),\n",
    "        'token_type_ids': batch['token_type_ids'].to(device),\n",
    "        \"attention_mask\": batch['attention_mask'].to(device),\n",
    "        \"start_positions\": batch['answer_start_token'].to(device),\n",
    "        \"end_positions\": batch['answer_end_token'].to(device),\n",
    "      }\n",
    "      \n",
    "      outputs = model(**inputs)\n",
    "      loss = outputs[0]\n",
    "      # Find the total loss\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    if (batch_idx+1) % print_every == 0:\n",
    "       print(\"Batch {:} / {:}\".format(batch_idx+1,len(val_loader)),\"\\nLoss:\", round(loss.item(),1),\"\\n\")\n",
    "\n",
    "  epoch_loss /= len(val_loader)\n",
    "  val_losses.append(epoch_loss)\n",
    "\n",
    "  # Print each epoch's time and train/val loss \n",
    "  print(\"\\n-------Epoch \", epoch+1,\n",
    "        \"-------\"\n",
    "        \"\\nTraining Loss:\", train_losses[-1],\n",
    "        \"\\nValidation Loss:\", val_losses[-1],\n",
    "        \"\\nTime: \",(time.time() - epoch_time),\n",
    "        \"\\n-----------------------\",\n",
    "        \"\\n\\n\")\n",
    "\n",
    "print(\"Total training and evaluation time: \", (time.time() - whole_train_eval_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(f\"pgajo/mbert-xl-wa-tasteset-recipe-aligner-{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(f\"pgajo/bert-base-multilingual-cased-word-align-{epoch}-multitraining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"pgajo/bert-base-multilingual-cased-word-align\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
