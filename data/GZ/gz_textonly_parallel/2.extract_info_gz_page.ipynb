{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/597 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 597/597 [01:48<00:00,  5.51it/s]\n",
      "100%|██████████| 597/597 [02:43<00:00,  3.66it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "db_dict = {\"en\": \"/home/pgajo/working/food/data/gz_textonly_parallel/gf_parallel/gf_en\",\n",
    "           \"it\": \"/home/pgajo/working/food/data/gz_textonly_parallel/gf_parallel/gf_it\"}\n",
    "\n",
    "for key in db_dict.keys():\n",
    "    if not os.path.exists(db_dict[key]):\n",
    "        os.mkdir(db_dict[key])\n",
    "\n",
    "def extract_recipe(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Print the title\n",
    "    title = soup.find('h1', class_=\"gz-title-recipe gz-mBottom2x\")\n",
    "    title_raw = f\"{title.get_text()}\\n{url}\"\n",
    "    # print(\"Title:\", title_raw)\n",
    "\n",
    "    main_content = soup.find_all('div', class_=\"gz-content-recipe gz-mBottom4x\")\n",
    "    presentation = main_content[0].find('p')\n",
    "    presentation_raw = presentation.get_text()\n",
    "    # print(\"Presentation:\", presentation_raw)\n",
    "\n",
    "    ingredients_div = soup.find('div', class_ = \"gz-ingredients gz-mBottom4x gz-outer\")\n",
    "    ingredient_list = []\n",
    "    for i, ingredient in enumerate(ingredients_div.find_all('dd', class_ = \"gz-ingredient\")):\n",
    "        ingredient_name = re.sub(\"\\s+\", \" \", ingredient.find('a').get_text())\n",
    "        # print(i, ingredient_name)\n",
    "        ingredient_quantity = re.sub(\"\\s+\", \" \", ingredient.find('span').get_text())\n",
    "        # print(i, ingredient_quantity)\n",
    "        # if len(' '.join(ingredient.find('span').get_text().split())) > 12:\n",
    "        #     print(f'Check length of quantity in {i}: {ingredient_quantity}')\n",
    "        line = ingredient_name + \"\\t\" + ingredient_quantity\n",
    "        ingredient_list.append(line)\n",
    "\n",
    "    ingredients_raw = '\\n'.join(ingredient_list)\n",
    "    # print(\"Ingredients:\")\n",
    "    # print(ingredients_raw.strip())\n",
    "        \n",
    "    preparation = main_content[1]\n",
    "    # print(preparation)\n",
    "\n",
    "    preparation_list = []\n",
    "    for paragraph in preparation.find_all('p'):\n",
    "        # print(paragraph)\n",
    "        for span in paragraph.find_all('span', class_=\"num-step\"):\n",
    "            step_number = span.get_text()\n",
    "            span.replace_with(f\"[{step_number}]\")\n",
    "        preparation_list.append(paragraph.get_text())\n",
    "    preparation_raw = '\\n'.join(preparation_list)\n",
    "    # print(\"Preparation:\", preparation_raw)\n",
    "\n",
    "    return title_raw, presentation_raw, ingredients_raw, preparation_raw\n",
    "\n",
    "df_recipes = pd.read_csv(\"/home/pgajo/working/food/data/gz_textonly_parallel/url_list.csv\")\n",
    "\n",
    "# let's only keep the ones which have both, so we can make a parallel corpus\n",
    "df_recipes = df_recipes.dropna()\n",
    "df_recipes = df_recipes.drop_duplicates()\n",
    "print(len(df_recipes))\n",
    "# # limit to 10 for testing purposes\n",
    "# df_recipes = df_recipes[:10]\n",
    "\n",
    "url_it_list = df_recipes['it']\n",
    "url_en_list = df_recipes['en']\n",
    "\n",
    "# create a new folder for the new english dataset and for the new italian dataset\n",
    " \n",
    "language_list = df_recipes.columns \n",
    "\n",
    "for lang in language_list:\n",
    "    \n",
    "    lang = lang.split(\"_\")[-1]\n",
    "    progressbar = tqdm(enumerate(df_recipes[lang]), total = len(df_recipes[lang]))#.set_description(lang)\n",
    "    for i, url in progressbar:\n",
    "        \n",
    "        title, presentation, ingredients, preparation = extract_recipe(url)\n",
    "        \n",
    "        recipe_path = os.path.join(db_dict[lang], f\"gf_{lang}_{i}\")\n",
    "        \n",
    "        if not os.path.exists(recipe_path):\n",
    "            os.mkdir(recipe_path)\n",
    "\n",
    "        # within recipe_path write a separate txt file for each output of extract_recipe\n",
    "\n",
    "        with open(os.path.join(recipe_path, \"title.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(title)\n",
    "\n",
    "        with open(os.path.join(recipe_path, \"presentation.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(presentation)\n",
    "\n",
    "        with open(os.path.join(recipe_path, \"ingredients.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(ingredients)\n",
    "\n",
    "        with open(os.path.join(recipe_path, \"preparation.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(preparation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://ricette.giallozafferano.it/Pan-di-Spagna.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Print the title\n",
    "title = soup.find('h1', class_=\"gz-title-recipe gz-mBottom2x\")\n",
    "title_raw = title.get_text()\n",
    "print(\"Title:\", title_raw)\n",
    "\n",
    "main_content = soup.find_all('div', class_=\"gz-content-recipe gz-mBottom4x\")\n",
    "presentation = main_content[0].find('p')\n",
    "presentation_raw = presentation.get_text()\n",
    "print(\"Presentation:\", presentation_raw)\n",
    "\n",
    "ingredients_div = soup.find('div', class_ = \"gz-ingredients gz-mBottom4x gz-outer\")\n",
    "ingredient_list = []\n",
    "for i, ingredient in enumerate(ingredients_div.find_all('dd', class_ = \"gz-ingredient\")):\n",
    "    ingredient_name = re.sub(\"\\s+\", \" \", ingredient.find('a').get_text())\n",
    "    # print(i, ingredient_name)\n",
    "    ingredient_quantity = re.sub(\"\\s+\", \" \", ingredient.find('span').get_text())\n",
    "    # print(i, ingredient_quantity)\n",
    "    if len(' '.join(ingredient.find('span').get_text().split())) > 12:\n",
    "        print(f'Check length of quantity in {i}!')\n",
    "    line = ingredient_name + \"\\t\" + ingredient_quantity\n",
    "    ingredient_list.append(line)\n",
    "\n",
    "ingredients_raw = '\\n'.join(ingredient_list)\n",
    "print(\"Ingredients:\")\n",
    "print(ingredients_raw.strip())\n",
    "    \n",
    "preparation = main_content[1]\n",
    "# print(preparation)\n",
    "\n",
    "preparation_list = []\n",
    "for paragraph in preparation.find_all('p'):\n",
    "    # print(paragraph)\n",
    "    for span in paragraph.find_all('span', class_=\"num-step\"):\n",
    "        step_number = span.get_text()\n",
    "        span.replace_with(f\"[{step_number}]\")\n",
    "    preparation_list.append(paragraph.get_text())\n",
    "preparation_raw = '\\n'.join(preparation_list)\n",
    "print(\"Preparation:\", preparation_raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
